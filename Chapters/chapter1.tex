%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter1.tex
%% NOVA thesis document file
%%
%% Chapter with Introduction
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter1.tex}%

\makeatletter
\newcommand{\ntifpkgloaded}{%
  \@ifpackageloaded%
}
\makeatother

%%\glsresetall
\chapter{Introduction}
\label{cha:introduction}

\textit{This section summarizes the study's underlying motivation, rationale, and goals, emphasizing its significance in the field. It provides context by giving some background on the supporting company and the initiative, along with other contributions. Furthermore, it outlines a reading guide for this thesis.}

\section{Significance and Objectives} % (fold)
\label{sec:significanceandobjectives}


\gls{DD} and development is a time-consuming, resource-intensive, multidisciplinary effort that can be challenging from many points of view. 
Over half of clinical trial failures are attributed to lack of efficacy, underscoring the importance of identifying and validating pharmacological targets, and highlighting the lack of knowledge of the drug's \gls{Mechanism of Action (MoA)} as one of the major barriers to clinical efficacy~\cite{RN3, RN1, RN2}. 
As thorough understanding of the \gls{MoA} is such a critical step, computational methods can accelerate the identification of pharmacologically active agents by providing more efficient and cost-effective alternatives to traditional approaches. 
Computational methods can accurately identify a new target or propose new indications for a known molecule, thereby reducing the time dedicated to \textit{in vitro} target validation~\cite{RN29}.

A key to understanding a compound's \gls{MoA} lies in transcriptomic profiling, which captures the changes in gene expression triggered by a perturbagen. 
While traditional \gls{RNA-seq} methods remain too costly for large-scale expression signatures, recent \gls{HTS} advances, such as the L1000 assay~\cite{RN30}, enable affordable generation and analysis of large-scale omics datasets. 
Several existing databases provide public access to transcriptomic data from experiments exposing cell lines to range of chemical and genetic perturbagens. 
These datasets can be leveraged, using various computational tools, to establish the causal chain of gene expression changes triggered by a specific compound. 
Three primary approaches have emerged: causal reasoning, connectivity mapping, and enrichment tools~\cite{RN38}.

Causal reasoning is a topology-based method, that determines potential causes for an observed gene expression profile, starting from a perturbation signature and a biological interaction network. 
The \gls{Molecular network} is defined as a signed and directed graph describing relations between nodes (e.g., proteins or genes). 
Efforts to compile causal molecular relation networks have increased, resulting in several publicly accessible databases (such as OmniPath), which offers curated \gls{PKN}. 
Among the networks commercially available~\cite{RN32} we can find MetaBase\textsuperscript{TM}, developed and curated by Clarivate.

The \gls{CMap} method is instead focused on collecting and analyzing perturbation signatures. 
In this case, a similarity score is used to compare a set of known \gls{MoA}/compound reference signatures, with a query gene expression profile from a perturbagen of interest~\cite{RN34, RN38}.
The principle behind \gls{CMap} is that the higher the similarity between the query and the reference signature, the more likely it is that the underlying mechanism is the same. 

On the other hand, enrichment tools take perturbation signatures as query input and utilize prior knowledge, such as a regulon network or collections of perturbation-induced \gls{DEGs}, as a reference. 
The purpose of these tools is to determine whether specific gene sets or regulons (genes interacting with \gls{TF}) are enriched in the perturbed data~\cite{RN35}.

With a systematic benchmarking approach, this study guides the selection of the most appropriate algorithms and inputs for evaluating the plethora of current solutions for \gls{MoA} reconstruction.
The evaluation process relies on three inputs:

\begin{itemize}
  \item \gls{Gene signature}s derived from chemical or genetic perturbation experiments.
  \item A \gls{PKN} of the molecular interactions within the system. 
  \item A gold standard dataset for validation of the results.
\end{itemize}

These data are used to feed the evaluated methods, serving as the reference, query, and gold standard datasets, respectively. 
This study analyzes the three types of tools depicted below: 

\begin{enumerate}
\item[\textbf{Topology-based tools}] Eight causal reasoning algorithms (\gls{CARNIVAL}, CausalR, \gls{ProTINA}, \gls{CIE}, NicheNet, plus causalReasoning, SigNet, quaternaryProd from the \gls{CBDD} \gls{R} package~\cite{RN36} , and five node prioritization algorithms (networkPropagation, randomWalk, overconnectivity, hiddenNodes, interconnectivity - from \gls{CBDD}). 
\item[\textbf{Similarity-based tools }] Six algorithms from the \gls{RCSM} R package.
\item[\textbf{Enrichment-based tools}] Eight algorithms from the decoupleR package~\cite{RN35}.
\end{enumerate}

Performance is assessed by comparing the results obtained against gold standard datasets, along with the robustness, and the computational efficiency. 
With this approach, the project aims to identify the most appropriate tools to contextualize gene expression data and if the choice of the inputs can also have an impact on the outcomes.

\section{Scope} % (fold)
\label{sec:scope}
This project was conducted within the framework of the \gls{ABC}, a subscription-based initiative for pharmaceutical companies led by Clarivate. \gls{ABC} is dedicated to evaluating a wide range of computational tools for a variety of applications in the life sciences and healthcare field. The topic for this thesis is the development of \gls{ABC}'s tenth use case - Causal Regulation - which focuses on benchmarking tools designed to identify key regulators from transcriptomic data and \gls{PKN}.

\section{Other Contributions} % (fold)
\label{sec:othercontributions}

This study expands the state of the art in causal reasoning using gene expression data and molecular interactions, by presenting a robust framework and a systematic algorithm benchmarking approach. The study was presented during the following poster communication:
\begin{enumerate}
\item[\textbf{XIV Edition of Bioinformatics Open Days}] M. Gomes, A. Ishkin, F. Ciceri, C. Klein. Benchmarking Computational Algorithms for enhanced Drug Discovery: Insights from Clarivate's pre-competitive Algorithm Benchmarking Consortium. XIV Edition of Bioinformatics Open Days, 26 March 2025, Braga, Portugal.
\end{enumerate}

Beyond the topic of this thesis, during the MSc industry placement, I was also involved as developer in other activities aimed at identifying reliable solutions for several different external stakeholders in the pharmaceutical business. Although not related with the topic of this thesis, these experiences enriched my consultant experience, allowing for an expansion in the expertise across various domains of computational biology and data science. 
These projects included:

\begin{enumerate}
\item[\textbf{Skin Microbiome Atlas}] This project involved the curation and re-analysis of publicly available skin microbiome datasets. My role in this project included conducting an in-depth scientific literature review, compiling relevant datasets, and pre-processing raw sequencing data to ensure consistency and comparability across studies. 
\item[\textbf{Natural Language Processing}] A pipeline was set up for automatic text classification of epidemiology abstracts, leveraging different versions of the BERT foundational model. Model fine-tuning on a minimal dataset was attempted by generating synthetic data using paraphrasing techniques. 
\item[\textbf{\gls{ABC} - Spatial Niche}] As part of an internal case study for \gls{ABC}, I implemented several Python wrappers for selected algorithms relevant to spatial niche analysis. My role in this use case mainly included the management of conda environments, the development of functions to run those algorithms and their integration with an \gls{R}-based pipeline.
\item[\textbf{Google Data Extraction Tool}] In collaboration with another team in the company, I developed an automated script for retrieving pharmacological information from different web sources. The pipeline involved querying URLs and keywords and extracting structured data through Google search API calls.
\item[\textbf{Transcriptomic comparative analysis}] I was involved in a project to perform a comparative analysis of transcriptomic profiles from three types of cancer. The workflow included exploratory data analysis, identification of \gls{DEGs} and hub genes, pathway enrichment analysis, causal reasoning to infer upstream regulators, and a survival analysis. The analysis also included the comparison of both the Human Papillomavirus status and tumor location.
\item[\textbf{Proteomics analysis}] In a proteomics-focused project, I was responsible for carrying out exploratory data analysis and functional enrichment analysis to uncover biological insights from protein-level data.
\item[\textbf{Single-cell atlas}] For the single-cell atlas project, I performed some downstream analysis, including enrichment analysis on a already built atlas. At this moment, I am involved in the construction of a new single-cell atlas, with the integration, pre-processing and quality control of datasets.
\item[\textbf{Indication-Prioritization}] This project involved the development of a pipeline to integrate disease and target data annotation from several sources, and prioritize disease indications based on customer's preference criteria.
\end{enumerate}

\section{Structure} % (fold)
\label{sec:structure}

This study\footnote{This work used generative artificial intelligence tools, namely \href{https://quillbot.com}{QuillBot}~\cite{QuillBot} and \href{https://copilot.microsoft.com/}{Microsoft Copilot}~\cite{Copilot}, to assist with text rewording. These tools were used under the author's supervision, and all generated content was checked for accuracy. Authorship and content validation remained the author's responsibility, and the use of artificial intelligence complies with institutional standards of academic integrity.} is organized in five chapters.
After this introductory chapter, Chapter~\ref{cha:literaturereview} starts by describing what \gls{MoA} is and its importance in \gls{DD}, followed by a review of the data and algorithms that can be used for \gls{MoA} reconstruction. This chapter also characterizes the role of a benchmarking study when there is a wide diversity of data and methods available.
Chapter~\ref{cha:materialsandmethods} describes the methodology used in this study, including the selection of gene expression datasets, \gls{Molecular network}s, and algorithms for benchmarking, the design of the experimental framework, and the evaluation metrics used to assess algorithm performance.
Chapter~\ref{cha:results} presents the results of the benchmarking, including the analysis of algorithm performance across different levels of the data.
In Chapter~\ref{cha:discussion} the results are interpreted, putting them in the context of the current literature in the field, and summarizes the key findings of the study and their significance, offering recommendations for future work.
At the end, Annex~\ref{ann:SupplementaryTables} includes the supplementary tables.
