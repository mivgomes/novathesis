%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter5.tex
%% NOVA thesis document file
%%
%% Chapter with a short latex tutorial and examples
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter5.tex}%


\chapter{Discussion}
\label{cha:discussion}


\textit{This chapter discusses the results by highlighting their implications and comparing them with the existing literature. Finally, the key findings are summarized into conclusions and suggestions for some future perspectives.}

The \gls{MoA} of a small molecule involves its interaction with specific molecular targets. When these primary targets are activated or inhibited, they trigger changes in downstream signaling pathways, modulating the activity of \gls{TF}s and therefore the expression of individual genes. 
All these changes will ultimately produce the desired drug effect together with any additional undesired side effects. 
As with many topics in the scientific community, there are different views on the real need of a complete understanding of the molecular target and the \gls{MoA} of a drug~\cite{RN112}. 
Some defend and prioritize clinical benefit, given the high number of drugs on the market for which the \gls{MoA} is currently unknown. 
On the other hand, some argue that understanding the effects of compounds is essential in the early stages of \gls{DD}. 
Knowing the \gls{MoA} of a drug not only helps to guide the process but also increases predictability and can even help to understand potential side effects. 
While understanding these mechanisms isn't strictly necessary for successful drug development, it plays a crucial role in improving efficiency. 
For this reason, advances in system biology are focused on reducing the process for \gls{MoA} reconstruction, keeping reliable results. 
For example, in 2020, the \gls{CMap} Institute created a challenge on the Kaggle platform~\cite{RN162} to develop an algorithm to predict the \gls{MoA} of a new drug. 
Along with these collective efforts, several bioinformatics tools have emerged to infer causal regulators. 
Authors who develop a new algorithm typically perform comparative benchmarking to highlight the novelty of their discovery~\cite{RN109}. 
However, these studies have often some limitations. The main one is that simulated data, such as the \gls{LINCS} dataset, is used instead of real experimental data. 
This approach may fail to capture the variability and complexity of biological systems, leading to a biased evaluation of the computation tool. Also, the datasets used are often small and curated, which may not reflect the challenges of real-world applications.
And finally, the evaluations often focus on a single aspect of performance, such as accuracy, without considering other important factors like scalability, robustness, and computational efficiency. 
Benchmarking studies are therefore essential because they aim to test various components in an unbiased and realistic way, to evaluate the performance of different classes of algorithms. 

This study represents the most comprehensive evaluation of \gls{MoA} recovery methods published to date. 
While previous studies have focused on specific categories of methods, limited network sources, or small selected datasets, this study covered 27 algorithms, including topology-based, enrichment, and \gls{CMap} methods, incorporating public and commercial reference datasets, and experimental and simulated data from seven distinct sources. 
This extends previous benchmarking efforts and shows critical differences between theory and practical feasibility.

Topology-based methods consistently achieved the best performance across all metrics in direct target retrieval. 
Consistent with other studies, randomWalk was at the top of all the rankings with the exception of runtime and pathway analysis.
In Hill \textit{et al.}~\cite{RN37}, node prioritization methods, including randomWalk followed by networkPropagation and GeneMANIA, were also identified as the top performers, using a combination of interactions from STRING, MetaBase, and BioPlex as reference dataset. 
Our study confirmed the superiority of randomWalk, together with evidences that performance depends on the choice of reference network, with the Metabase network providing the best results, probably due to its greater molecular coverage.

The similar performance of randomWalk with different reference datasets demonstrate the robustness and reliability of the algorithm. 
The overconnectivity and networkPropagation algorithms also performed well in our study. 
However, each one had some limitations in terms of accuracy, reliability, or run time. 
For example, networkPropagation obtained results similar to randomWalk in terms of \gls{AUC}, but with reduced reliability in large-scale executions, suggesting possible concerns with scalability.

Our study reveals an important trade-off between direct recovery and the biological context, not previously characterized in other studies. 
Methods that achieved the highest pathway enrichment scores, such as wmean and wsum, also obtained negative \gls{WPAE} values, indicating that they do not outperform direct target classification methods. 
On the other hand, although randomWalk performs well in target identification, it seems to compromise pathway enrichment performance, by obtaining half the scores of wmean and wsum. 
This inverse relationship suggests that algorithms designed to capture biological context end up sacrificing accuracy in identifying individual causal nodes. 
As a consequence, methods selection goes beyond simple performance rankings, as it should also account the need to a deeper understanding of the biological context surrounding a specific target. 
If understanding the broader biological context is more important than identifying single targets, these enrichment methods can provide better results, despite lower accuracy. 
In contrast, when experimental validation resources limit testing to a few candidates, the accuracy of randomWalk seems to be more appropriate.

A study by Lin, K., \textit{et al.}~\cite{RN79} and others [108] identified ZhangScore as the best algorithm among \gls{CMap} methods. 
However, in our study, this algorithm performed poorly. 
In general, all \gls{CMap} algorithm achieved low \gls{AUC} values (< 0.08), win rate values (< 0.03), and pathway enrichment scores (0.20  0.31). 
Among the \gls{CMap} algorithms, GSEAweight1 showed the best results, followed by \gls{KS}, with Zhang only distinguishing itself because of the worst computational efficiency, having the highest runtime. 
This huge difference compared with published results can be interpreted considering the task being evaluated. 
While the studies mentioned above were assessing the similarity between signatures, in this study we are evaluating the ability to identify the true target responsible for the perturbation signatures. 
These results suggest that similarity-based algorithm fails to capture the causal regulatory relationships underlying perturbation responses, meaning that gene expression pattern relationship does not necessarily reflects shared regulatory mechanism.

Consistent with Hill \textit{et al.}~\cite{RN37}, we observed that causal reasoning methods performance is better at pathway-level enrichment, compared with precise target identification. 
For example, causalReasoning achieved perfect reliability, but modest \gls{AUC}, in line with the observation that these methods struggle to pinpoint exact drug targets. 
This pattern was observed for SigNet and CausalR [27] and it can reflect that many regulatory targets are not \gls{TF}s or are not directly observable in expression data. 
The most interesting and surprising finding from this study is the issue of scalability for causal reasoning algorithms, which resulted in complete failure of the most sophisticated among them (\gls{CARNIVAL}). 
In previous benchmarking studies, SigNet and CARNIVAL had the best performance [27], with OmniPath network as reference data. 
However, despite \gls{CARNIVAL}'s theoretical ability for capturing complex regulatory interactions, its computational requirements make it unusable for realistic dataset sizes. 
This emphasizes that algorithm benchmarking must consider not just accuracy but also scalability and runtimes, that are often neglected when evaluations focus on small and curated datasets. 
In this large-scale evaluation several computationally intensive algorithms (e.g., NicheNet, ProTINA) are impractical for high-throughput use, with runtimes exceeding 400 seconds per signature or total failure to complete the analyses. 
For methods intended for integration into large-scale screening pipelines, these constraints represent a critical limitation.

With a strong influence on algorithm performance, the importance of network choice was no exception in our study. 
Metabase demonstrated the most consistent target recovery results, while others appear better suited for pathway enrichment. 
Network selection should be aligned with the research goal, with denser and high-coverage networks for precision target recovery and more structured, curated pathway resources for context mapping.

The consistent superiority of drug perturbations over genetic approaches is important to highlight. 
The assumption that genetic perturbations are more precise can be refuted by our results, where well-designed drug perturbations seem to provide better results. 
Also, the poor performance of \gls{LINCS} genetic perturbations highlights potential limitations of cell line-based genetic screens.

The validation approaches used by the original dataset creators varied considerably, affecting result interpretation, as a consequence the quality of the datasets can be a determining factor in the performance of these tools. 
Among the 7 sources of datasets used, based on the validation carried out by the authors, some datasets do not seem to be the most suitable for obtaining reliable results. 
\gls{CREEDS} is one such case where the data is extracted from public databases and not generated by the researchers involved in the benchmarking effort~\cite{RN87}. 
The data is only checked on a technical level, to confirm that the samples are from \gls{GEO}, and if they are labeled correctly. 
The attempt at biological evaluation, by looking for specific patterns of signatures (i.e. if two signatures come from perturbing the same gene), showed inconsistent results. 
Moreover, for some datasets validation was practically non-existent~\cite{RN85, RN86}, and the filters used during the pre-processing step of this study actually increased the quality of the data (this is the case of ChemPert and PertOrg). 
Both sources for the single-cell datasets showed a good validation of the data. GWPS, focused on gene knockdown signatures, used strict criteria to define significant responses~\cite{RN89}. 
Sci-Plex, with drug perturbations, validated its data through some complementary statistical analysis~\cite{RN88}. 
The data from \gls{LINCS} are more controversial. 
First, L1000 measures around 978 landmark genes and computationally infers the rest (~12,000). 
Additionally, despite each perturbation is tested in triplicate and z-scores are adjusted to minimize replicate inconsistencies~\cite{RN30}, the reliability of these data is still questionable. 
Recent alternatives, such as DRUG-seq, demonstrate to have some advantages over L1000. 
Direct comparison between the two datasets~\cite{RN130, RN105} showed that DRUG-seq directly measures more than 10,000 genes without relying on inference, at a lower cost. 
When testing the accuracy of both, DRUG-Seq proved to be more accurate in distinguishing samples between different diseases. 
Also, the emergence of Perturb-seq, which combines \gls{CRISPR} perturbations with single-cell RNA-seq~\cite{RN89}, offers another promising alternative with greater transcriptome coverage at a reduced cost~\cite{RN165}. 
Finally, the cancer cell line composition is another shortcoming of this dataset limiting applicability to other contexts. 
This is being addressed by programs such as Neuro\gls{LINCS}~\cite{RN164}, which create signatures using patient-derived induced pluripotent stem cells~\cite{RN165}. 
Our finding that tissue-derived signatures consistently outperformed cell line data supports the expansion of data derived from other sources.

The critical importance of reference network selection extends beyond simple coverage metrics. 
Large-scale networks may include interactions irrelevant to specific cellular contexts~\cite{RN38}. 
For this reason, tools for constraining networks based on tissue-specific expression data from Human Protein Atlas~\cite{RN166} or TissueNet~\cite{RN137} could improve performance~\cite{RN38}. 
Our results showing MetaBase Linear Path Regulons' excellent pathway enrichment, despite poor coverage, support this tissue-specific approach.

The focus on transcriptomic-based methods, in this thesis, but also other benchmarking studies, represents just one facet of \gls{MoA} inference. 
Recent multi-OMICS integration tools like SignalingProfiler 2.0~\cite{RN100} and COSMOS~\cite{RN99} demonstrate the value of combining transcriptomics with proteomics, metabolomics, and phosphoproteomics data. 
COSMOS's successful application to capture relevant crosstalks within and between multiple OMICS layers, resulting in identification of known clear cell renal cell carcinoma drug targets, illustrates how the integration of multiple data types can generate novel biomarker hypotheses. 
Additionally, data capturing changes in cell morphology and the chemical structure of specific compounds~\cite{RN167} can become a valuable complement to expression data in \gls{DD} studies~\cite{RN38}. 

Some considerations and future work that can be explored are related with certain adjustments or experimental directions that could have been considered, but have been left out of this study for time and resource limitations. 
One aspect that has not been considered, but could effectively influence the performance of the algorithms is parameter optimization. 
Algorithm behavior and performance can change dramatically with parameter tuning, and it is suggested as good practice for benchmarking studies~\cite{RN108}. 
Yet comprehensive optimization across all algorithm-dataset combinations proved computationally unreasonable. 
The same applies to the methods for filtering \gls{DEGs} from full signatures. 
However, the parameters of the algorithms and the preparation of the input data are specified and calculated in the wrapper function and they can be changed if needed. 
Being this a benchmarking study, the entire workflow is set up precisely to test multiple options, even though not all of them have been included in this work. 
When evaluating, another metric that could be included was an ensemble score. 
Instead of relying on a single tool or data set, a score combining the results of several tools could also be generated to produce more robust predictions. 
Future benchmarking efforts can also weight results by validation confidence, given that the heterogeneity in dataset validation can partially explain some performance variations across query datasets.

The \gls{Molecular network}s used as a reference in this study, both MetaBase and OmniPath, are of considerable size, with hundreds of thousands of interactions. 
When using these networks, one of the recommendations is to ensure that the interactions present are within the context of the study, in this case, the type of cell or tissue being studied~\cite{RN38}. 
This reasoning makes sense if we consider that these large networks include all kinds of interactions, including those specific to other cells or tissues. 
To this end, instead of using global networks, more specific and context-relevant networks can be used, which may already be prepared by the databases themselves. 
Alternatively, databases such as TissueNet~\cite{RN137}, provide interaction networks that are specific to certain tissues. 
Another thing that could be considered would be to integrate networks from different sources, for example, MetaBase and OmniPath networks, customizing the subsets of interactions to suit the context and by keeping only those with higher reliability scores. 
If the focus extends beyond the types of algorithms used in this project, additional software and tools are available for target identification that leverage different computational approaches. 
Depending on the specific scientific questions being investigated, it could be also worth exploring them.
One example is Drug2ways~\cite{RN132}, a software implemented in Python, to identify potential drug repositioning and predict the effects of drugs. 
To do this, it performs causal reasoning through biological networks with three types of vertices: molecular entities, drugs, and indications. 
The paths between the drug and the indication are noted, as well as their direction, suggesting positive or negative regulation. The most frequent direction is taken as the predicted effect. 
Solutions are becoming increasingly innovative, taking advantage of advances in computer efficiency. 
Such as the use of knowledge graphs and Large language models for drug repurposing~\cite{RN163}. 

In summary, based on the results obtained and the intersection of previous research, this study recommends the randomWalk algorithm for precise target recovery, as well as wmean for pathway-level context. 
It is important to emphasize the importance of high-quality reference data, and for topology-based tools, to consider filtering interactions relevant to the study in question. 
Regarding query data, the use of experimental data and data from tissues should be preferred. 
In addition, other algorithm optimization parameters should be considered, rather than the default settings. 
Finally, given the progresses in the field of system biology and the avalanche of increasingly available data, integration of other levels of OMICS data and the use of machine learning methods promise to further improve the ability to identify target genes and perturbed pathways.
