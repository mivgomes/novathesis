%!TEX root = ../template.tex
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% chapter5.tex
%% NOVA thesis document file
%%
%% Chapter with a short latex tutorial and examples
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\typeout{NT FILE chapter5.tex}%


\chapter{Discussion}
\label{cha:discussion}


\textit{This chapter discusses the results by highlighting their implications and comparing them with the existing literature. Finally, the key findings are summarized into conclusions and suggestions for some future perspectives.}

The MoA of a small molecule involves its interaction with specific molecular targets. When these primary targets are activated or inhibited, they trigger changes in downstream signaling pathways, including the activity of transcription factors and thus the expression of the genes. All these changes that happen in the system will ultimately produce the desired (or undesired) drug effect. As with many topics in the scientific community, there are different views on the real need to know the exact molecular target or MoA of a drug's function [106]. Some defend and prioritize the practical results of drugs, given the effective number of drugs on the market for which the MoA is unknown. On the other hand, some argue that understanding the effects of compounds is essential in the early stages of DD. Knowing the MoA of a drug not only helps to guide the process but also increases predictability and can even help to understand potential side effects. While understanding these mechanisms isn't strictly necessary for successful drug development, it plays a crucial role in improving efficiency. Given the benefits, advances in this area are moving towards finding methods that accelerate this time-consuming process while maintaining reliable results. For example, in 2020, the Connectivity Mapping Institute created a challenge on the Kaggle platform [107] to develop an algorithm that would predict the MoA of a new drug. Along with these collective efforts, several bioinformatics tools have emerged to infer causal regulators. Authors who develop a new algorithm typically evaluate and compare it with others [95]. The aim is to demonstrate that the algorithm outperforms others already existing. However, these studies have some limitations. Typically, simulated data, such as the LINCS dataset, is used instead of real experimental data. Results from this simulated data may be unreliable because the variability and complexity of biological systems are not fully assessed. Benchmarking studies are therefore essential because they aim to test various components in an unbiased and realistic way to evaluate the performance of algorithms. 
This study represents the most comprehensive evaluation of MoA recovery methods published to date. While previous studies have focused on specific categories of methods, limited network sources, or small selected datasets, this study covered 27 algorithms, including topology-based, enrichment, and connectivity mapping methods, incorporating public and commercial reference datasets, and experimental and simulated data from seven distinct sources. This extends previous benchmarking efforts and shows critical differences between theory and practical feasibility.
Topology-based methods consistently achieved the best performance across all metrics in direct target retrieval. randomWalk was almost always at the top of the ranking for AUC, reliability, and win rate, as well as run times of around 5 seconds per signature. These results are consistent with other studies. In Hill et al. [60], node prioritization methods, including randomWalk followed by networkPropagation and GeneMANIA, were also identified as the top performers. Our study demonstrated that performance depends on the choice of reference network, with the MetaBase network providing the best results, probably due to its greater molecular coverage. Hill et al. used a combination of interactions from STRING, MetaBase, and BioPlex.  This variation in topological networks while maintaining the same performance suggests that randomWalk is a robust and reliable algorithm. The overconnectivity and networkPropagation algorithms also performed well in our study. However, each had some limitations in terms of accuracy, reliability, or run time. For example, networkPropagation obtained results like randomWalk in AUC, but with reduced reliability in large-scale executions, suggesting possible concerns with scalability.
Our study reveals an important trade-off between direct recovery and the biological context that previous studies did not characterize. Methods that achieved the highest pathway enrichment scores, such as wmean and wsum, also obtained negative WPAE values, indicating that they do not outperform direct target classification methods. On the other hand, although randomWalk performs well in target identification, it seems to compromise pathway enrichment performance, by obtaining half the scores of wmean and wsum. This inverse relationship suggests that algorithms designed to capture biological context end up sacrificing accuracy in identifying individual causal nodes. The selection of methods goes beyond simple performance rankings. When understanding the broader biological context is more important than identifying single targets, these enrichment methods can provide better results, despite lower accuracy. In contrast, when experimental validation resources limit testing to a few candidates, the accuracy of randomWalk seems to be more appropriate.
A study by Lin, K., et al [87] and others [108] identified ZhangScore as the best algorithm among connectivity mapping methods. However, in our study, this algorithm performed poorly among all. In general, all CMap algorithm achieved low AUC values (< 0.08), win rate values (< 0.03), and pathway enrichment scores (0.20 – 0.31). Among the CMap algorithms, GSEAweight1 showed the best results, followed by KS, with Zhang only distinguishing itself through the worst computational efficiency, by having the highest runtime. This huge difference in the results can be interpreted as the actual task being evaluated. While the studies mentioned above were assessing the similarity between signatures, in this study we are evaluating the ability to identify the true target responsible for the perturbation signatures. These results suggest that similarity-based algorithm fails to capture the causal regulatory relationships underlying perturbation responses, meaning that gene expression pattern relationship does not necessarily reflects shared regulatory mechanism.
Consistent with Hill et al [27], we observed that causal reasoning methods performance better at pathway-level enrichment compared with precise target identification. For example, causalReasoning achieved perfect reliability, but modest AUC, aligning with the observation that these methods struggle to pinpoint exact drug targets directly. This pattern was observed for SigNet and CausalR [27] and it can reflect that many regulatory targets are not TFs or are not directly observable in expression data. The most interesting and surprising finding with causal reasoning algorithms in regard with other studies, was scalability. With the complete failure of the most sophisticated algorithms, such as, CARNIVAL. Especially, when these algorithms performed well. In previous benchmarking studies SigNet and CARNIVAL with OmniPath network as reference data had the best performance [27]. However, while CARNIVAL's theoretical ability for capturing complex regulatory interactions, its computational requirements make it unusable for realistic dataset sizes. This emphasizes that algorithm benchmarking must consider not just accuracy but also scalability and runtimes, that are often neglected when evaluations focus on small and curated datasets. In this large-scale evaluation several computationally intensive algorithms (e.g., NicheNet, ProTINA) are impractical for high-throughput use, with runtimes exceeding 400 seconds per signature or failing to complete analyses. For methods intended for integration into large-scale screening pipelines, these constraints represent a critical limitation.
With a strong influence on algorithm performance, the importance of network choice was no exception in our study. Metabase demonstrated the most consistent target recovery results, while others appear better suited for pathway enrichment. Network selection should be aligned with the research goal, with denser and high-coverage networks for precision target recovery and more structured, curated pathway resources for context mapping.
The consistent superiority of drug perturbations over genetic approaches is important to highlight. The assumption that genetic perturbations are more precise can be refuted by our results. Well-designed drug perturbations seem to provide better results. Also, the poor performance of LINCS genetic perturbations highlights potential limitations of cell line–based genetic screens.
The validation approaches used by the original dataset creators varied considerably, affecting result interpretation. The quality of the datasets can be a determining factor in the performance of these tools. The truth is that, among the 7 sources of datasets used, based on the validation carried out by the authors, some datasets do not seem to be the most suitable for obtaining reliable results. CREEDS is one such case where the data is extracted from public databases and not generated by them [51]. The data is only checked on a technical level, if the samples really come from GEO, and if the samples are labeled correctly. The attempt at biological evaluation, by looking for specific patterns of signatures, if two signatures come from perturbing the same gene, showed inconsistent results. Moreover, there are certain datasets in which the authors' validation was practically non-existent [39, 56], but the filters used in this study to prepare the dataset increased the quality of the data, as was the case for ChemPert and PertOrg. Both sources for the single-cell datasets showed a good validation of the data. GWPS, focused on gene knockdown signatures, used strict criteria to define significant responses [54]. Sci-Plex, with drug perturbations, validated its data through some complementary statistical analysis [42]. The data from LINCS are more controversial. First, L1000 measures around 978 landmark genes and computationally infers the rest (~12,000). And although each perturbation is tested in triplicate, and z-scores are adjusted to minimize replicate inconsistencies [40], the reliability of these data is questionable. Recent alternatives, such as DRUG-seq, demonstrate advantages. One study compared L1000 data with DRUG-Seq data [52, 53]. DRUG-seq directly measures more than 10,000 genes without relying on inference, including several not covered by L1000, at a lower cost. When testing the accuracy of both, DRUG-Seq proved to be more accurate in distinguishing samples between different diseases. Also, the emergence of Perturb-seq, which combines CRISPR perturbations with single-cell RNA-seq [54], offers another promising alternative with greater transcriptome coverage at a reduced cost [109]. Also, the cancer cell line composition limits LINCS's applicability to other contexts. This is being addressed by programs such as NeuroLINCS [110], which create signatures using patient-derived induced pluripotent stem cells [109]. Our finding that tissue-derived signatures consistently outperformed cell line data supports the expansion of data derived from other sources.
The critical importance of reference network selection extends beyond simple coverage metrics. Large-scale networks may include interactions irrelevant to specific cellular contexts [28]. Tools for constraining networks based on tissue-specific expression data from resources like the Human Protein Atlas [111] or specialized databases like TissueNet [65] could improve performance [28]. Our results showing MetaBase Linear Path Regulons' excellent pathway enrichment despite poor coverage support this tissue-specific approach.
The focus on transcriptomics-based methods, in this thesis, but also other benchmarking studies, represents just one facet of MoA inference. Recent multi-OMICS integration tools like SignalingProfiler 2.0 [85] and COSMOS [112] demonstrate the value of combining transcriptomics with proteomics, metabolomics, and phosphoproteomics data. COSMOS's successful application to capture relevant crosstalks within and between multiple omics layers, such as known clear cell renal cell carcinoma drug targets, illustrates how the integration of multiple data types can generate novel biomarker hypotheses. Data capturing changes in cell morphology after perturbation, and the chemical structure of compounds [113] can become a valuable complement to expression data in DD studies [28]. 
Some considerations and future work that can be explored have to do with certain adjustments or paths that could be taken, but which, due to the size of the project for this phase, were not possible to include. One aspect that has not been considered in this study, but which could effectively influence the behavior and therefore the performance of the algorithms, is the optimization of the algorithm parameters. Algorithm behavior and performance can change dramatically with parameter tuning, and it is suggested as good practice for benchmarking studies [94]. Yet comprehensive optimization across all algorithm-dataset combinations proved computationally unreasonable for this study. The same goes for the methods for filtering datasets when the algorithms required DEG, and the input query data was a full signature. Other filtering methods can be tested. The parameters of the algorithms and the preparation of the input data are defined and prepared in the wrapper functions to be changed if needed. As this is a benchmarking study, the entire workflow is set up precisely to test multiple options, even though not all of them have been evaluated in this work. Regarding evaluation, another metric that could be included was an ensemble score. Instead of relying on a single tool or data set, a score combining the results of several tools can also be generated to produce more robust predictions. Future benchmarking efforts can also weight results by validation confidence, given that the heterogeneity in dataset validation can partially explain some performance variations across query datasets.
The molecular networks used as a reference in this study, both MetaBase and OmniPath, are of considerable size, with hundreds of thousands of interactions. When using these networks, one of the recommendations is to ensure that the interactions present are within the context of the study, in this case, the type of cell or tissue being studied [28]. This reasoning makes sense if we consider that these large networks include all kinds of interactions, including those specific to other cells or tissues. To this end, instead of using global networks, more specific and context-relevant networks can be used, which may already be prepared by the databases themselves. Alternatively, use databases such as TissueNet [65], which provides interaction networks that are specific to certain tissues. Another thing that could be considered would be to integrate networks from different sources, for example, MetaBase and Network. And following the reasoning, it could be customized to suit the context and even keep only highly reliable interactions. 
If we do not just focus on this type of algorithm, there are more software and interesting tools with a similar purpose, but with different reasoning, that may be worth exploring too, always depending on the question we want to answer. One example is Drug2ways [114], a software implemented in Python, to identify potential drug repositioning and predict the effects of drugs. To do this, it performs causal reasoning through biological networks with three types of vertices: molecular entities, drugs, and indications. The paths between the drug and the indication are noted, as well as their direction, suggesting positive or negative regulation. The most frequent direction is taken as the predicted effect. Solutions are becoming increasingly innovative, taking advantage of advances in computer efficiency. Such as the use of knowledge graphs and Large language models for drug repurposing [115]. 
In summary, based on the results obtained and the intersection of previous studies, this study recommends the randomWalk algorithm for precise target recovery, as well as wmean for pathway-level context. It is important to emphasize the importance of high-quality reference data, and for topology-based tools, to consider filtering interactions relevant to the study in question. Regarding query data, the use of experimental data and data from tissues should be considered. In addition, other algorithm optimization parameters should be considered, rather than sticking to the default settings. Finally, with the evolution of all these areas and the avalanche of increasingly available data, the integration of other levels of OMICS data and even the use of machine learning methods should be considered.
